{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiniRedTrout/NeuroLess/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Датасет"
      ],
      "metadata": {
        "id": "VdvU3fa8NleV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsbF_NHZfQhE",
        "outputId": "bd2cf9d6-e9c6-4296-9a5d-888621976032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'mushrooms-classification-common-genuss-images' dataset.\n",
            "Path to dataset files: /kaggle/input/mushrooms-classification-common-genuss-images\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"maysee/mushrooms-classification-common-genuss-images\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импорт библиотек"
      ],
      "metadata": {
        "id": "hGIqQyo_NoPK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "btJvNFS0fVEn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "import timm\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from torch import nn\n",
        "from torchmetrics import Accuracy\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader,Subset\n",
        "from torchvision import transforms, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QsCPhF0gFuk",
        "outputId": "38c0889b-59c7-4bac-9e81-f52aa389fdab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Аугментация"
      ],
      "metadata": {
        "id": "uYoN3tLDNzMg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "g9RuU8jJgKPG"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224,scale=(0.8,1.0)),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.RandomVerticalFlip(0.3),\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "        transforms.GaussianBlur(kernel_size=3,sigma=(0.1,2.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3))\n",
        "        ])\n",
        "basic_transforms = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "O42dbPqplr2I"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.ImageFolder(path+'/Mushrooms',transform=train_transforms)\n",
        "\n",
        "targets = np.array([sample[1] for sample in dataset.samples])\n",
        "\n",
        "train_idx, test_idx = train_test_split(\n",
        "    np.arange(len(targets)),\n",
        "    test_size=0.2,\n",
        "    stratify=targets,\n",
        "    random_state=42\n",
        "  )\n",
        "train_dataset = Subset(dataset, train_idx)\n",
        "test_dataset = Subset(dataset, test_idx)\n",
        "train_dataset.dataset.transform = train_transforms\n",
        "test_dataset.dataset.transform = basic_transforms\n",
        "class_names = dataset.classes[0:9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "S50oCgjlmxD5"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset,batch_size=64)\n",
        "test_loader = DataLoader(test_dataset,batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель"
      ],
      "metadata": {
        "id": "Z-tFluUYOEsF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6Ix95xbm0D1",
        "outputId": "eedfdf18-1d4f-4c9b-9912-628de4c5f797"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.3, inplace=False)\n",
              "    (4): Linear(in_features=512, out_features=9, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "from torchvision.models import ResNet18_Weights\n",
        "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "\n",
        "model.fc =  nn.Sequential(\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(model.fc.in_features, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(512, 9)\n",
        "    )\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "WTUgL3DY0GSD"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "accuracy = Accuracy(task='multiclass', num_classes=9).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "3BRioj2xw3UT"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "#Тренирует\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc='Training')\n",
        "    for batch_idx, (data, target) in enumerate(progress_bar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        all_preds.append(predicted)\n",
        "        all_targets.append(target)\n",
        "        progress_bar.set_postfix({\n",
        "            'Loss': f'{loss.item():.4f}',\n",
        "            'Avg Loss': f'{running_loss/(batch_idx+1):.4f}'\n",
        "        })\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = accuracy(torch.cat(all_preds), torch.cat(all_targets))\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "#Тестит\n",
        "def evaluate(model, dataloader, criterion, device,matrix=0):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in tqdm(dataloader, desc='Evaluating'):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            all_preds.append(predicted.cpu())\n",
        "            all_targets.append(target.cpu())\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = accuracy(torch.cat(all_preds), torch.cat(all_targets))\n",
        "    if matrix != 0:\n",
        "        all_preds2 = np.concatenate(all_preds)\n",
        "        all_targets2 = np.concatenate(all_targets)\n",
        "        print()\n",
        "        print(classification_report(all_targets2, all_preds2, target_names=class_names))\n",
        "    return epoch_loss, epoch_acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 7"
      ],
      "metadata": {
        "id": "vXkyNAoS_u_W"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMePoR5w1Oga",
        "outputId": "a50d60d5-fa34-462b-9a52-8150d8835565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Эпоха 1/7\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 84/84 [01:08<00:00,  1.23it/s, Loss=1.4776, Avg Loss=1.7244]\n",
            "Evaluating: 100%|██████████| 21/21 [00:14<00:00,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.7244, Train Acc: 0.4074\n",
            "Test Loss: 1.2069, Test Acc: 0.6188\n",
            "\n",
            "Эпоха 2/7\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 84/84 [01:07<00:00,  1.25it/s, Loss=0.8313, Avg Loss=1.0164]\n",
            "Evaluating: 100%|██████████| 21/21 [00:14<00:00,  1.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0164, Train Acc: 0.6773\n",
            "Test Loss: 0.7876, Test Acc: 0.7483\n",
            "\n",
            "Эпоха 3/7\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 84/84 [01:07<00:00,  1.25it/s, Loss=0.4174, Avg Loss=0.5905]\n",
            "Evaluating: 100%|██████████| 21/21 [00:15<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5905, Train Acc: 0.8304\n",
            "Test Loss: 0.5632, Test Acc: 0.8168\n",
            "\n",
            "Эпоха 4/7\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 84/84 [01:08<00:00,  1.23it/s, Loss=0.2077, Avg Loss=0.2974]\n",
            "Evaluating: 100%|██████████| 21/21 [00:15<00:00,  1.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2974, Train Acc: 0.9313\n",
            "Test Loss: 0.4714, Test Acc: 0.8451\n",
            "\n",
            "Эпоха 5/7\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 84/84 [01:08<00:00,  1.22it/s, Loss=0.1040, Avg Loss=0.1412]\n",
            "Evaluating: 100%|██████████| 21/21 [00:14<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1412, Train Acc: 0.9752\n",
            "Test Loss: 0.4266, Test Acc: 0.8585\n",
            "\n",
            "Эпоха 6/7\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 84/84 [01:09<00:00,  1.20it/s, Loss=0.0742, Avg Loss=0.0736]\n",
            "Evaluating: 100%|██████████| 21/21 [00:14<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0736, Train Acc: 0.9935\n",
            "Test Loss: 0.4170, Test Acc: 0.8637\n",
            "\n",
            "Эпоха 7/7\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 84/84 [01:09<00:00,  1.21it/s, Loss=0.0685, Avg Loss=0.0649]\n",
            "Evaluating: 100%|██████████| 21/21 [00:14<00:00,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0649, Train Acc: 0.9952\n",
            "Test Loss: 0.4135, Test Acc: 0.8637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    print(f\"\\nЭпоха {epoch+1}/{epochs}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc.cpu())\n",
        "\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_acc.cpu())\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "Ip6c4Djl1UvF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "555508ae-5804-4a11-c59e-b0dd9313a8da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 21/21 [00:14<00:00,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Agaricus       0.75      0.69      0.72        70\n",
            "     Amanita       0.90      0.86      0.88       150\n",
            "     Boletus       0.90      0.96      0.93       215\n",
            " Cortinarius       0.87      0.80      0.83       167\n",
            "    Entoloma       0.82      0.85      0.83        73\n",
            "   Hygrocybe       0.94      0.94      0.94        63\n",
            "   Lactarius       0.84      0.90      0.87       313\n",
            "     Russula       0.89      0.85      0.87       230\n",
            "     Suillus       0.79      0.71      0.75        62\n",
            "\n",
            "    accuracy                           0.86      1343\n",
            "   macro avg       0.85      0.84      0.85      1343\n",
            "weighted avg       0.86      0.86      0.86      1343\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Оценка\n",
        "final_test_loss, final_test_acc = evaluate(model, test_loader, criterion, device, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "gDORx2n3GJAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f210630-d6d0-44a8-e0b0-ba70339f16ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Финальный скор:0.8637\n"
          ]
        }
      ],
      "source": [
        "print(f'Финальный скор:{final_test_acc.cpu():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "N22lCH5nGK6T"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'test_accuracy': final_test_acc,\n",
        "    'test_loss': final_test_loss\n",
        "}, 'resnet_mushrooms_model.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPOfW2uxkLhftTlKpabPCTF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}