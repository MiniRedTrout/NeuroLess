{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1shg1g2mHNvd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self,data):\n",
        "    super(Model, self).__init__()\n",
        "    self.lstm_size = 128\n",
        "    self.embedding_dim = 128\n",
        "    self.num_layers = 3\n",
        "\n",
        "    n_vocab = len(data.uniq_words)\n",
        "    self.embedding = nn.Embedding(\n",
        "        num_embeddings=n_vocab,\n",
        "        embedding_dim=self.embedding_dim\n",
        "    )\n",
        "    self.lstm = nn.LSTM(\n",
        "        input_size=self.lstm_size,\n",
        "        hidden_size=self.lstm_size,\n",
        "        num_layers=self.num_layers,\n",
        "        dropout=0.2\n",
        "    )\n",
        "    self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "  def forward(self,x,prev_state):\n",
        "    embed = self.embedding(x)\n",
        "    output, state = self.lstm(embed,prev_state)\n",
        "    logits = self.fc(output)\n",
        "    return logits,state\n",
        "  def init_state(self, sequence_length):\n",
        "    return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),torch.zeros(self.num_layers, sequence_length, self.lstm_size))\n"
      ],
      "metadata": {
        "id": "1nF6Y4GSUq6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,args):\n",
        "    self.args = args\n",
        "    self.words = self.load_words()\n",
        "    self.uniq_words = self.get_uniq_words()\n",
        "\n",
        "    self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "    self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "\n",
        "    self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "  def load_words(self):\n",
        "    train_df = pd.read_csv('/content/reddit-cleanjokes.csv')\n",
        "    text = train_df['Joke'].str.cat(sep=' ')\n",
        "    return text.split(' ')\n",
        "  def get_uniq_words(self):\n",
        "    word_counts = Counter(self.words)\n",
        "    return sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "  def __len__(self):\n",
        "    return len(self.words_indexes) - self.args.sequence_length\n",
        "  def __getitem__(self, index):\n",
        "    return (\n",
        "        torch.tensor(self.words_indexes[index:index+self.args.sequence_length]),\n",
        "        torch.tensor(self.words_indexes[index+1:index+self.args.sequence_length+1])\n",
        "    )"
      ],
      "metadata": {
        "id": "P9qIC9wRUrWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train(dataset, model, args):\n",
        "    model.train()\n",
        "    loss_epochs = []\n",
        "    dataloader = DataLoader(dataset, batch_size=args.batch_size)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    for epoch in range(args.max_epochs):\n",
        "        loss_epoch = 0\n",
        "        state_h, state_c = model.init_state(args.sequence_length)\n",
        "        loader = tqdm(dataloader, desc=f'Epoch{epoch}/{args.max_epochs}')\n",
        "        for batch, (x, y) in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "            loss = criterion(y_pred.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_epoch += loss\n",
        "            index = batch + 1\n",
        "        loss_epoch = loss_epoch/index\n",
        "        loss_epochs.append(loss_epoch)\n",
        "        loader.set_postfix(\n",
        "            {\n",
        "                'Loss': loss_epoch\n",
        "            }\n",
        "        )"
      ],
      "metadata": {
        "id": "LUV_krH7SB1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(dataset, model, text, next_words=100):\n",
        "    model.eval()\n",
        "\n",
        "    words = text.split(' ')\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "    for i in range(0, next_words):\n",
        "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(dataset.index_to_word[word_index])\n",
        "\n",
        "    return words"
      ],
      "metadata": {
        "id": "XfPb6iM9SFnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    sequence_length = 10\n",
        "    batch_size = 64\n",
        "    max_epochs = 50\n",
        "\n",
        "dataset = Dataset(Args())\n",
        "model = Model(dataset)"
      ],
      "metadata": {
        "id": "Q1oNEWzvSQwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(dataset,model,Args())"
      ],
      "metadata": {
        "id": "YAGlBoeRST6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23b7d3c-aed6-4f1d-ec09-dac1d23726e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch0/50: 100%|██████████| 374/374 [01:08<00:00,  5.48it/s]\n",
            "Epoch1/50: 100%|██████████| 374/374 [01:07<00:00,  5.57it/s]\n",
            "Epoch2/50: 100%|██████████| 374/374 [01:08<00:00,  5.47it/s]\n",
            "Epoch3/50: 100%|██████████| 374/374 [01:09<00:00,  5.39it/s]\n",
            "Epoch4/50: 100%|██████████| 374/374 [01:07<00:00,  5.53it/s]\n",
            "Epoch5/50: 100%|██████████| 374/374 [01:08<00:00,  5.46it/s]\n",
            "Epoch6/50: 100%|██████████| 374/374 [01:08<00:00,  5.43it/s]\n",
            "Epoch7/50: 100%|██████████| 374/374 [01:08<00:00,  5.50it/s]\n",
            "Epoch8/50: 100%|██████████| 374/374 [01:09<00:00,  5.40it/s]\n",
            "Epoch9/50: 100%|██████████| 374/374 [01:09<00:00,  5.40it/s]\n",
            "Epoch10/50: 100%|██████████| 374/374 [01:09<00:00,  5.41it/s]\n",
            "Epoch11/50: 100%|██████████| 374/374 [01:09<00:00,  5.42it/s]\n",
            "Epoch12/50: 100%|██████████| 374/374 [01:08<00:00,  5.46it/s]\n",
            "Epoch13/50: 100%|██████████| 374/374 [01:11<00:00,  5.26it/s]\n",
            "Epoch14/50: 100%|██████████| 374/374 [01:09<00:00,  5.39it/s]\n",
            "Epoch15/50: 100%|██████████| 374/374 [01:09<00:00,  5.38it/s]\n",
            "Epoch16/50: 100%|██████████| 374/374 [01:09<00:00,  5.38it/s]\n",
            "Epoch17/50: 100%|██████████| 374/374 [01:09<00:00,  5.40it/s]\n",
            "Epoch18/50: 100%|██████████| 374/374 [01:08<00:00,  5.44it/s]\n",
            "Epoch19/50: 100%|██████████| 374/374 [01:09<00:00,  5.36it/s]\n",
            "Epoch20/50: 100%|██████████| 374/374 [01:09<00:00,  5.40it/s]\n",
            "Epoch21/50: 100%|██████████| 374/374 [01:08<00:00,  5.49it/s]\n",
            "Epoch22/50: 100%|██████████| 374/374 [01:08<00:00,  5.42it/s]\n",
            "Epoch23/50: 100%|██████████| 374/374 [01:10<00:00,  5.29it/s]\n",
            "Epoch24/50: 100%|██████████| 374/374 [01:09<00:00,  5.36it/s]\n",
            "Epoch25/50: 100%|██████████| 374/374 [01:09<00:00,  5.37it/s]\n",
            "Epoch26/50: 100%|██████████| 374/374 [01:08<00:00,  5.45it/s]\n",
            "Epoch27/50: 100%|██████████| 374/374 [01:08<00:00,  5.47it/s]\n",
            "Epoch28/50: 100%|██████████| 374/374 [01:06<00:00,  5.60it/s]\n",
            "Epoch29/50: 100%|██████████| 374/374 [01:07<00:00,  5.51it/s]\n",
            "Epoch30/50: 100%|██████████| 374/374 [01:06<00:00,  5.59it/s]\n",
            "Epoch31/50: 100%|██████████| 374/374 [01:07<00:00,  5.54it/s]\n",
            "Epoch32/50: 100%|██████████| 374/374 [01:06<00:00,  5.59it/s]\n",
            "Epoch33/50: 100%|██████████| 374/374 [01:07<00:00,  5.51it/s]\n",
            "Epoch34/50: 100%|██████████| 374/374 [01:07<00:00,  5.57it/s]\n",
            "Epoch35/50: 100%|██████████| 374/374 [01:06<00:00,  5.60it/s]\n",
            "Epoch36/50: 100%|██████████| 374/374 [01:07<00:00,  5.50it/s]\n",
            "Epoch37/50: 100%|██████████| 374/374 [01:06<00:00,  5.59it/s]\n",
            "Epoch38/50: 100%|██████████| 374/374 [01:08<00:00,  5.50it/s]\n",
            "Epoch39/50: 100%|██████████| 374/374 [01:07<00:00,  5.56it/s]\n",
            "Epoch40/50: 100%|██████████| 374/374 [01:06<00:00,  5.61it/s]\n",
            "Epoch41/50: 100%|██████████| 374/374 [01:09<00:00,  5.42it/s]\n",
            "Epoch42/50: 100%|██████████| 374/374 [01:08<00:00,  5.47it/s]\n",
            "Epoch43/50: 100%|██████████| 374/374 [01:08<00:00,  5.45it/s]\n",
            "Epoch44/50: 100%|██████████| 374/374 [01:08<00:00,  5.43it/s]\n",
            "Epoch45/50: 100%|██████████| 374/374 [01:08<00:00,  5.43it/s]\n",
            "Epoch46/50: 100%|██████████| 374/374 [01:10<00:00,  5.31it/s]\n",
            "Epoch47/50: 100%|██████████| 374/374 [01:10<00:00,  5.29it/s]\n",
            "Epoch48/50: 100%|██████████| 374/374 [01:09<00:00,  5.36it/s]\n",
            "Epoch49/50: 100%|██████████| 374/374 [01:09<00:00,  5.37it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(dataset,model,text='Knock knock. Whos there?')"
      ],
      "metadata": {
        "id": "lNRWbdG3AlTd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e504b702-ab9f-46e7-dceb-e63c6c5666b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Knock',\n",
              " 'knock.',\n",
              " 'Whos',\n",
              " 'there?',\n",
              " 'Do',\n",
              " 'you',\n",
              " 'bury',\n",
              " 'wear',\n",
              " 'Honey.',\n",
              " \"They're\",\n",
              " 'full',\n",
              " 'What',\n",
              " 'did',\n",
              " 'one',\n",
              " 'Schwarzenegger',\n",
              " 'than',\n",
              " 'when',\n",
              " 'they',\n",
              " \"couldn't\",\n",
              " 'arrested?',\n",
              " 'Well,',\n",
              " 'I',\n",
              " 'just',\n",
              " 'got',\n",
              " 'a',\n",
              " 'lot!',\n",
              " 'joke',\n",
              " 'on',\n",
              " 'no',\n",
              " 'whale',\n",
              " 'standards',\n",
              " 'who?',\n",
              " '...and',\n",
              " 'one',\n",
              " 'tie',\n",
              " 'out',\n",
              " 'to',\n",
              " 'the',\n",
              " 'top!',\n",
              " 'What',\n",
              " 'mysterious',\n",
              " 'pf',\n",
              " 'ducks',\n",
              " 'do',\n",
              " 'always',\n",
              " 'payed',\n",
              " 'for',\n",
              " 'eight',\n",
              " 'own',\n",
              " 'whale,',\n",
              " 'Why',\n",
              " 'did',\n",
              " 'the',\n",
              " 'puppy',\n",
              " 'get',\n",
              " 'over?',\n",
              " 'getting',\n",
              " 'other',\n",
              " 'Just',\n",
              " 'said',\n",
              " 'to',\n",
              " 'a',\n",
              " 'foreign',\n",
              " 'planed',\n",
              " 'your',\n",
              " 'Wild',\n",
              " '***P***',\n",
              " 'walks',\n",
              " 'into',\n",
              " 'a',\n",
              " 'bar...',\n",
              " 'Which',\n",
              " 'asks',\n",
              " 'you',\n",
              " 'goes',\n",
              " 'to',\n",
              " '2',\n",
              " 'person',\n",
              " 'Apple',\n",
              " 'grass',\n",
              " 'is',\n",
              " 'no',\n",
              " 'corner!',\n",
              " 'on',\n",
              " 'fire',\n",
              " 'What',\n",
              " 'did',\n",
              " 'the',\n",
              " 'fish',\n",
              " 'say',\n",
              " 'when',\n",
              " 'it',\n",
              " \"couldn't\",\n",
              " 'out',\n",
              " 'the',\n",
              " 'window?',\n",
              " 'From',\n",
              " 'her',\n",
              " 'love',\n",
              " 'look',\n",
              " 'broke',\n",
              " 'by',\n",
              " 'all',\n",
              " 'before']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gPKg-VIByiF1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}