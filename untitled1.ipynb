{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiniRedTrout/NeuroLess/blob/main/untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Датасет"
      ],
      "metadata": {
        "id": "VdvU3fa8NleV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsbF_NHZfQhE",
        "outputId": "bd2cf9d6-e9c6-4296-9a5d-888621976032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'mushrooms-classification-common-genuss-images' dataset.\n",
            "Path to dataset files: /kaggle/input/mushrooms-classification-common-genuss-images\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"maysee/mushrooms-classification-common-genuss-images\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импорт библиотек"
      ],
      "metadata": {
        "id": "hGIqQyo_NoPK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btJvNFS0fVEn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "import timm\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from torch import nn\n",
        "from torchmetrics import Accuracy\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader,Subset\n",
        "from torchvision import transforms, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QsCPhF0gFuk",
        "outputId": "38c0889b-59c7-4bac-9e81-f52aa389fdab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Аугментация"
      ],
      "metadata": {
        "id": "uYoN3tLDNzMg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9RuU8jJgKPG"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224,scale=(0.8,1.0)),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.RandomVerticalFlip(0.3),\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "        transforms.GaussianBlur(kernel_size=3,sigma=(0.1,2.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3))\n",
        "        ])\n",
        "basic_transforms = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O42dbPqplr2I"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.ImageFolder(path+'/Mushrooms',transform=train_transforms)\n",
        "\n",
        "targets = np.array([sample[1] for sample in dataset.samples])\n",
        "\n",
        "train_idx, test_idx = train_test_split(\n",
        "    np.arange(len(targets)),\n",
        "    test_size=0.2,\n",
        "    stratify=targets,\n",
        "    random_state=42\n",
        "  )\n",
        "train_dataset = Subset(dataset, train_idx)\n",
        "test_dataset = Subset(dataset, test_idx)\n",
        "train_dataset.dataset.transform = train_transforms\n",
        "test_dataset.dataset.transform = basic_transforms\n",
        "class_names = dataset.classes[0:9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S50oCgjlmxD5"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset,batch_size=64)\n",
        "test_loader = DataLoader(test_dataset,batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель"
      ],
      "metadata": {
        "id": "Z-tFluUYOEsF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6Ix95xbm0D1",
        "outputId": "d231db84-7b6a-4996-a351-53a8cad0ced0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.3, inplace=False)\n",
              "    (4): Linear(in_features=512, out_features=9, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "model.fc =  nn.Sequential(\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(model.fc.in_features, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(512, 9)\n",
        "    )\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTUgL3DY0GSD"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "accuracy = Accuracy(task='multiclass', num_classes=9).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BRioj2xw3UT"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "#Тренирует\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc='Training')\n",
        "    for batch_idx, (data, target) in enumerate(progress_bar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        all_preds.append(predicted)\n",
        "        all_targets.append(target)\n",
        "        progress_bar.set_postfix({\n",
        "            'Loss': f'{loss.item():.4f}',\n",
        "            'Avg Loss': f'{running_loss/(batch_idx+1):.4f}'\n",
        "        })\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = accuracy(torch.cat(all_preds), torch.cat(all_targets))\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "#Тестит\n",
        "def evaluate(model, dataloader, criterion, device,matrix=0):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in tqdm(dataloader, desc='Evaluating'):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            all_preds.append(predicted.cpu())\n",
        "            all_targets.append(target.cpu())\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = accuracy(torch.cat(all_preds), torch.cat(all_targets))\n",
        "    if matrix != 0:\n",
        "        all_preds2 = np.concatenate(all_preds)\n",
        "        all_targets2 = np.concatenate(all_targets)\n",
        "        print()\n",
        "        print(classification_report(all_targets2, all_preds2, target_names=class_names))\n",
        "    return epoch_loss, epoch_acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 7"
      ],
      "metadata": {
        "id": "vXkyNAoS_u_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMePoR5w1Oga",
        "outputId": "5c00d4d2-96a1-4739-a780-c9ad90977f98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Эпоха 1/7\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 84/84 [01:07<00:00,  1.25it/s, Loss=1.4486, Avg Loss=1.7467]\n",
            "Evaluating: 100%|██████████| 21/21 [00:14<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.7467, Train Acc: 0.3876\n",
            "Test Loss: 1.2178, Test Acc: 0.6180\n",
            "\n",
            "Эпоха 2/7\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 84/84 [01:08<00:00,  1.23it/s, Loss=0.8465, Avg Loss=1.0382]\n",
            "Evaluating: 100%|██████████| 21/21 [00:14<00:00,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0382, Train Acc: 0.6695\n",
            "Test Loss: 0.7857, Test Acc: 0.7506\n",
            "\n",
            "Эпоха 3/7\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 84/84 [01:08<00:00,  1.23it/s, Loss=0.4565, Avg Loss=0.6106]\n",
            "Evaluating: 100%|██████████| 21/21 [00:14<00:00,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6106, Train Acc: 0.8205\n",
            "Test Loss: 0.5658, Test Acc: 0.8161\n",
            "\n",
            "Эпоха 4/7\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 84/84 [01:08<00:00,  1.23it/s, Loss=0.2059, Avg Loss=0.3166]\n",
            "Evaluating: 100%|██████████| 21/21 [00:14<00:00,  1.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3166, Train Acc: 0.9229\n",
            "Test Loss: 0.4660, Test Acc: 0.8474\n",
            "\n",
            "Эпоха 5/7\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 84/84 [01:08<00:00,  1.23it/s, Loss=0.0984, Avg Loss=0.1463]\n",
            "Evaluating: 100%|██████████| 21/21 [00:14<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1463, Train Acc: 0.9754\n",
            "Test Loss: 0.4283, Test Acc: 0.8548\n",
            "\n",
            "Эпоха 6/7\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 84/84 [01:08<00:00,  1.23it/s, Loss=0.0669, Avg Loss=0.0778]\n",
            "Evaluating: 100%|██████████| 21/21 [00:15<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0778, Train Acc: 0.9935\n",
            "Test Loss: 0.4209, Test Acc: 0.8578\n",
            "\n",
            "Эпоха 7/7\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 84/84 [01:09<00:00,  1.22it/s, Loss=0.0704, Avg Loss=0.0709]\n",
            "Evaluating: 100%|██████████| 21/21 [00:14<00:00,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0709, Train Acc: 0.9940\n",
            "Test Loss: 0.4196, Test Acc: 0.8608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    print(f\"\\nЭпоха {epoch+1}/{epochs}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc.cpu())\n",
        "\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_acc.cpu())\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip6c4Djl1UvF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c922522d-4dd7-4f1d-e184-7d02606ebeaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 21/21 [00:14<00:00,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Agaricus       0.76      0.79      0.77        70\n",
            "     Amanita       0.90      0.85      0.87       150\n",
            "     Boletus       0.88      0.96      0.92       215\n",
            " Cortinarius       0.86      0.81      0.84       167\n",
            "    Entoloma       0.79      0.78      0.79        73\n",
            "   Hygrocybe       0.93      0.89      0.91        63\n",
            "   Lactarius       0.88      0.89      0.88       313\n",
            "     Russula       0.86      0.85      0.85       230\n",
            "     Suillus       0.74      0.69      0.72        62\n",
            "\n",
            "    accuracy                           0.86      1343\n",
            "   macro avg       0.84      0.84      0.84      1343\n",
            "weighted avg       0.86      0.86      0.86      1343\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Оценка\n",
        "final_test_loss, final_test_acc = evaluate(model, test_loader, criterion, device, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDORx2n3GJAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c51161f3-33c8-441d-ccc8-fbf0d2dc415e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Финальный скор:0.8608\n"
          ]
        }
      ],
      "source": [
        "print(f'Финальный скор:{final_test_acc.cpu():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N22lCH5nGK6T"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'test_accuracy': final_test_acc,\n",
        "    'test_loss': final_test_loss\n",
        "}, 'resnet_mushrooms_model.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMjmQsLeFXFvYn6A0uoOP9a",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}